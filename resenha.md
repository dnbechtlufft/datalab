The author begins the article by explaining the concepts of representation, evaluation, and optimization, however when examples are used, containing combinations of each of these concepts, the explanation gets confusing. The use of Barebone Decision-Tree algorithm as an illustration without a specific explanation might scare the beginners, but may also spark some curiosity to accomplish a deeper understanding of it.

Another text's failure is the section about theoretical. There's not enough explanation for the equations used, and unfortunately, the entire paragraph is based on these. As a result, this section affords to explain easily only one easy concept, that's not always we can trust in theoretics. 

Meanwhile, the concepts of generalization and overfitting became very educational. The graphics were very helpful, and the examples used were way friendlier than the previous ones. The author also shows some methods that seem useful and efficient to avoid overfitting, such as regularization, cross-validation and feed more input data to the model.

While remaining educational, the text is accurate, giving an initial idea of the importance of using features adjusted in the best way possible. It's not only about having a lot of data but having it processed accurately. Nevertheless, the author alert that a dumb algorithm sometimes can beat a clever one if the first has a lot more data.

Continuing, the following chapters alert about two typical mistakes: become an expert in one algorithm neglecting the others, and choose a model based just on its complexity. Learning other algorithms should fix both, and with time and practice in these, you will know what model uses each time.

To conclude, the author clarifies two commons misunderstandings: even if you can represent a problem perhaps your model couldn't learn it, this can occur because you have little data or you're using the wrong model; the second warns that a feature might be correlational to the output, but it wouldn't the cause of the result.